{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ec67c49-f40d-40ca-8ac7-cf3c463232c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "s = \"2025-01-01 12:00:00 +03:00\"  # preferred normalized form\n",
    "df = spark.range(1).select(F.to_timestamp(F.lit(s), \"yyyy-MM-dd HH:mm:ss XXX\").alias(\"ts\"))\n",
    "\n",
    "# ------------\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "df.show(truncate=False)\n",
    "# ------------\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"America/Toronto\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "# ------------\n",
    "s = \"2025-01-01 12:00:00 +03:00\"  # preferred normalized form\n",
    "df = spark.range(1).select(F.to_utc_timestamp(F.to_timestamp(F.lit(s), \"yyyy-MM-dd HH:mm:ss XXX\"),\"America/Toronto\").alias(\"ts\"))\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe336a30-6e5e-4057-b718-9114d7f8c681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ts_str = \"2022-01-01 00:30:00\"\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "spark.range(1).select(F.to_date(F.lit(ts_str).cast(\"timestamp\")).alias(\"day\")).show()\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"America/Toronto\")\n",
    "spark.range(1).select(F.to_date(F.lit(ts_str).cast(\"timestamp\")).alias(\"day\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "970538a0-2593-443f-b28e-f31719a2d5b5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756057673219}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, StringType, DateType, TimestampNTZType, StructField, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "schema = StructType([ StructField(\"user_id\",IntegerType(), True),\n",
    "             StructField(\"timestamp\",StringType(), True),\n",
    "             StructField(\"page\",StringType(), True),\n",
    "             StructField(\"duration_seconds\",IntegerType(), True)\n",
    "            \n",
    "            ] )\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(\n",
    " [  (1,\"2022-01-01 12:00:00\",\"home\",30),\n",
    "    (2,\"2022-01-01 12:05:00\",\"dashboard\",45),\n",
    "    (3,\"2022-01-01 12:10:00\",\"profile\",60),\n",
    "    (1,\"2022-01-01 12:15:00\",\"home\",20),\n",
    "    (2,\"2022-01-01 12:20:00\",\"profile\",30),\n",
    "    (3,\"2022-01-01 12:25:00\",\"dashboard\",40)],\n",
    " schema =   schema\n",
    ")\n",
    "\n",
    "\n",
    "w = Window.partitionBy(\"page\").orderBy(\"timestamp\")\n",
    "\n",
    "# per-row window sum (same sum repeated on every row in that page)\n",
    "with_sum = df.withColumn(\n",
    "    \"sum_per_page\",\n",
    "    F.sum(\"duration_seconds\").over(w)\n",
    ") # .withColumn(\"rn\", F.row_number().over(w))\n",
    "\n",
    "\n",
    "display( with_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cc07023-6ed8-4f96-8d55-288c54670fa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/csv/user_engagement.csv\")\n",
    "\n",
    "display( df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6f9d112-6f97-48dc-a6ed-d3a8c903eea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tests/test_user_engagement_dq_string_ts.py\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "PATTERN = \"yyyy-MM-dd HH:mm:ss\"\n",
    "MIN_DT = \"1900-01-01 00:00:00\"\n",
    "MAX_DT = \"2999-12-31 23:59:59\"\n",
    "\n",
    "def _sample_df(spark):\n",
    "    data = [\n",
    "        (1, \"2022-01-01 12:00:00\", \"home\", 30),\n",
    "        (2, \"2022-01-01 12:05:00\", \"dashboard\", 45),\n",
    "        (3, \"2022-01-01 12:10:00\", \"profile\", 60),\n",
    "        (1, \"2022-01-01 12:15:00\", \"home\", 20),\n",
    "        (2, \"2022-01-01 12:20:00\", \"profile\", 30),\n",
    "        (3, \"2022-01-01 12:25:00\", \"dashboard\", 40),\n",
    "    ]\n",
    "    return spark.createDataFrame(data, [\"user_id\",\"timestamp\",\"page\",\"duration_seconds\"])\n",
    "\n",
    "def test_not_nulls(spark):\n",
    "    df = _sample_df(spark)\n",
    "    null_viol = df.filter(\n",
    "        F.col(\"user_id\").isNull() |\n",
    "        F.col(\"timestamp\").isNull() |\n",
    "        F.col(\"page\").isNull() |\n",
    "        F.col(\"duration_seconds\").isNull()\n",
    "    )\n",
    "    assert null_viol.count() == 0, \"Nulls found in required columns\"\n",
    "\n",
    "def test_user_id_and_duration_min_values(spark):\n",
    "    df = _sample_df(spark)\n",
    "    viol = df.filter(\n",
    "        (F.col(\"user_id\") < -1) | (F.col(\"duration_seconds\") < -1)\n",
    "    )\n",
    "    assert viol.count() == 0, \"Values < -1 detected in user_id/duration_seconds\"\n",
    "\n",
    "def test_timestamp_format_and_parseable(spark):\n",
    "    df = _sample_df(spark)\n",
    "    # parse string → timestamp using the required pattern\n",
    "    parsed = df.withColumn(\"ts_parsed\", F.to_timestamp(\"timestamp\", PATTERN))\n",
    "    bad_fmt = parsed.filter(F.col(\"ts_parsed\").isNull())\n",
    "    assert bad_fmt.count() == 0, f\"Timestamp not matching pattern {PATTERN}\"\n",
    "\n",
    "def test_timestamp_in_range(spark):\n",
    "    df = _sample_df(spark)\n",
    "    parsed = df.withColumn(\"ts_parsed\", F.to_timestamp(\"timestamp\", PATTERN))\n",
    "    viol = parsed.filter(\n",
    "        (F.col(\"ts_parsed\") < F.to_timestamp(F.lit(MIN_DT))) |\n",
    "        (F.col(\"ts_parsed\") > F.to_timestamp(F.lit(MAX_DT)))\n",
    "    )\n",
    "    assert viol.count() == 0, \"Timestamps out of allowed range [1900-01-01, 2999-12-31]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e2476a-7210-432d-9673-b6694394889c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93f3b1c9-375a-4209-a4d5-4e4e32bc174e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, shutil, pytest\n",
    "\n",
    "SRC_TEST = \"/Workspace/Users/andrewkravchuk@outlook.com/Gore Mutual/tests/test_basic.py\"\n",
    "DST_DIR  = \"/tmp/tests\"\n",
    "DST_TEST = f\"{DST_DIR}/test_basic.py\"\n",
    "\n",
    "os.makedirs(DST_DIR, exist_ok=True)\n",
    "shutil.copy(SRC_TEST, DST_TEST)\n",
    "\n",
    "# Add -W filters (you can stack multiple)\n",
    "exit_code = pytest.main([\n",
    "    DST_TEST,\n",
    "    \"-v\",\n",
    "    \"-rP\",                        # ← extra summary for Passed tests\n",
    "    \"-o\", \"cache_dir=/tmp/pytest_cache\",\n",
    "    \"-W\", \"ignore:distutils Version classes are deprecated:DeprecationWarning\",\n",
    "    \"-W\", \"ignore::DeprecationWarning:pyspark.sql.pandas.utils\",\n",
    "])\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7329099232723195,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Gore Home Assignment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
